---
title: "GroupB_Final"
author: "S.Carpenè, G.Fantuzzi, V.Nigam, M.Tallone, A.Valentinis"
date: "2024-02-14"
output:
  html_document:
    toc: yes
    toc_depth: '3'
    df_print: paged
  pdf_document:
    toc: yes
    toc_depth: 3
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries_and_utilities, echo=FALSE, warning=FALSE, message=FALSE}
# Loading the utilities for libraries, assessment and plots
source("markdown_utils.r")

# Eventual additional options
options(rgl.useNULL=TRUE)

# Loading the dataset for eda plots

# Set working directory as this directory
setwd(dirname(rstudioapi::getSourceEditorContext()$path))

# Load the dataset from the datasets/ folder
bank <- read.csv("./datasets/BankChurners.csv", sep = ",")

# ⚠️ Remove the first and last two columns as suggested in the README
bank <- bank[, -c(1, 22, 23)]

# ⚠️ Convert the Attrition_Flag column to a binary variable:
# - 0: Existing Customer
# - 1: Attrited Customer
bank$Attrition_Flag <- ifelse(bank$Attrition_Flag == "Attrited Customer", 1, 0)

# Convert all categorical variables to factors
bank$Gender <- as.factor(bank$Gender)
bank$Education_Level <- as.factor(bank$Education_Level)
bank$Marital_Status <- as.factor(bank$Marital_Status)
bank$Income_Category <- as.factor(bank$Income_Category)
bank$Card_Category <- as.factor(bank$Card_Category)

# FIlter numerical variables and categorical variables
bank_num <- bank[, sapply(bank, is.numeric)]
bank_cat <- bank[, sapply(bank, is.factor)]

# Bank Logistic dataset

# Load the dataset and pre-process it
bank_logistic <- read.csv("./datasets/BankChurners.csv", sep = ",")
bank_logistic <- bank_logistic[, -c(1, 3, 5, 6, 9, 10, 14, 16, 17, 21, 22, 23)]
bank_logistic$Attrition_Flag <- ifelse(bank_logistic$Attrition_Flag == "Attrited Customer", 1, 0)

# Convert all categorical variables to factors and reorder the levels
bank_logistic$Gender <- as.factor(bank_logistic$Gender)
bank_logistic$Marital_Status <- as.factor(bank_logistic$Marital_Status)
bank_logistic$Marital_Status <- forcats::fct_relevel(bank_logistic$Marital_Status,
                                            "Unknown",
                                            "Single",
                                            "Married",
                                            "Divorced")

# Changing the levels of Marital_Status in either married or not married
bank_logistic$Marital_Status <- fct_collapse(bank_logistic$Marital_Status,
                                    "Married" = c("Married"),
                                    "Not Married" = c("Divorced",
                                                      "Single",
                                                      "Unknown"))

bank_logistic$Income_Category <- as.factor(bank_logistic$Income_Category)
bank_logistic$Income_Category <- forcats::fct_relevel(bank_logistic$Income_Category,
                                             "Unknown",
                                             "Less than $40K",
                                             "$40K - $60K",
                                             "$60K - $80K",
                                             "$80K - $120K",
                                             "$120K +")

# Changing the levels of income category into a binary variable:
bank_logistic$Income_Category <- fct_collapse(bank_logistic$Income_Category,
                                     "Less than 120K" = c("Unknown",
                                                          "Less than $40K",
                                                          "$40K - $60K",
                                                          "$60K - $80K",
                                                          "$80K - $120K"),
                                     "More than 120K" = c("$120K +"))

# Override the Total_Trans_Amt variable with its log !!!
bank_logistic$Total_Trans_Amt <- log(bank_logistic$Total_Trans_Amt)

# Standardization (optional) all columns except response and categorical
bank_logistic[, -c(1, 2, 3, 4)] <- scale(bank_logistic[, -c(1, 2, 3, 4)])

# ROSE
bank_logistic_balanced<- ROSE(Attrition_Flag~.,data=bank_logistic,seed = 123)$data
```

# Problem statement and dataset

The `BankChurners` dataset contains information about a bank's costumers and their credit card usage. The dataset is available on the Kaggle website and it is used for a classification problem. The main goal is to predict the `Attrition_Flag` response variable, which basically explains whether a customer will churn or not. The dataset consist of 10127 observations and 21 variables.

# Data preparation and EDA (Exploratory Data Analysis)

With the exclusion of the first variable (`CLIENTNUM`), that is just an identification number, and the last two (`Naive_Bayes_Classifier...`) variables, the dataset contains 19 possible covariates to be used for prediction. Among these, there are 6 categorical variables and 13 numerical variables.

The `Attrition_Flag` variable is the response variable and it is a binary variable. As it's possible to see form the following barplot, the dataset is unbalanced, with only 16.07% of the observations being positive, i.e. a churned costumer, for the `Attrition_Flag` variable.

```{r attrition_flag_barplot, echo=FALSE, fig.width=8, fig.height=5}
p1 <- ggplot(bank, aes(x = as.factor(Attrition_Flag))) +
    geom_bar(aes(fill = as.factor(Attrition_Flag)), color = "#FFFFFF") +
    scale_fill_manual(values = c("royalblue", "#FF5733"),
                      name = "Attrition Flag:",
                      labels = c("Existing", "Attrited")) +
    geom_text(aes(label = after_stat(count),
                  y = after_stat(count)),
              stat = "count",
              vjust = -0.5,
              size = 10) +
    labs(x = "Attrition Flag", y = "Count") +
    ggtitle("Response variable: Attrition_Flag") +
    theme(legend.position = c(.85, .85),
          legend.background = element_rect(fill = "transparent"),
          legend.direction = "vertical",
          legend.title = element_text(size = 10),
          aspect.ratio = 1) +
    ylim(0, 10000)

p2 <- ggplot(bank_logistic_balanced, aes(x = as.factor(Attrition_Flag))) +
    geom_bar(aes(fill = as.factor(Attrition_Flag)), color = "#FFFFFF") +
    scale_fill_manual(values = c("royalblue", "#FF5733"),
                      name = "Attrition Flag:",
                      labels = c("Existing", "Attrited")) +
    geom_text(aes(label = after_stat(count),
                  y = after_stat(count)),
              stat = "count",
              vjust = -0.5,
              size = 10) +
    labs(x = "Attrition Flag", y = "Count") +
    ggtitle("Response variable: Attrition_Flag") +
    theme(legend.position = c(.85, .85),
          legend.background = element_rect(fill = "transparent"),
          legend.direction = "vertical",
          legend.title = element_text(size = 10),
          aspect.ratio = 1) +
    ylim(0, 10000)

p1 | p2
```

```{r,echo=FALSE}
cat("Proportion of attrited (original dataset):",
    sum(bank$Attrition_Flag==1)/sum(table(bank_logistic$Attrition_Flag))*100,"%")
cat("Proportion of attrited (ROSE):",
    sum(bank_logistic_balanced$Attrition_Flag==1)/sum(table(bank_logistic$Attrition_Flag))*100,"%")
```

To check for possible correlation between the variables, we computed the correlation matrix and we plotted it using a heatmap. The correlation matrix is the following:

<!-- ```{r correlation_matrix, echo=FALSE} -->
```{r correlation_matrix, message=FALSE, echo=FALSE, fig.width=10, fig.height=10}
# Compute the correlation matrix
corm <- bank_num |>
  corrr::correlate() |>
  corrr::shave(upper = FALSE)


# Pivot the matrix and fix the labels
corm <- corm |>
  tidyr::pivot_longer(
    cols = -term,
    names_to = "colname",
    values_to = "corr"
  ) |>
  dplyr::mutate(
    rowname = forcats::fct_inorder(term),
    colname = forcats::fct_inorder(colname),
    label = dplyr::if_else(is.na(corr), "", sprintf("%1.2f", corr))
  )

# Plot the correlation matrix
ggplot(corm, aes(rowname, fct_rev(colname),
                 fill = corr)) +
  geom_tile() +
  geom_text(aes(
    label = label,
    color = abs(corr) < .75
  )) +
  coord_fixed(expand = FALSE) +
  scale_color_manual(
    values = c("white", "black"),
    guide = "none"
  ) +
  scale_fill_distiller(
    palette = "RdYlBu", na.value = "white",
    direction = -1, limits = c(-1, 1),
    name = "Pearson\nCorrelation:"
  ) +
  labs(x = NULL, y = NULL) +
  theme(panel.border = element_rect(color = NA, fill = NA),
        legend.position = c(.85, .8),
        axis.text.x = element_text(angle = 50, vjust = 1, hjust = 1))
```

As it's possible to see from the first column, the majority of the variables are not highly correlated with the response variable, with the exception of the `Total_Trans_Ct` variable that has a correlation of 0.37. However, an important thing to notice is that the correlation matrix shows some variables that are highly correlated with each other. Among these we can highlight:

* the `Avg_Open_To_Buy` and `Credit_Limit` variables, which are totally correlated;
* the `Months_on_book` and `Customer_Age` variables, that are reasonably correlated with a correlation of 0.79: indeed one expects that the number of months a customer has been with the bank is related to the customer's age;
* the `Total_Trans_Amt` and `Total_Trans_Ct` variables, that show a correlation of 0.81;
* the `Total_Revolving_Bal` and `Avg_Utilization_Ratio` variables, that are also highly correlated with a correlation of 0.62.

Since both the `Total_Trans_Amt` and `Total_Trans_Ct` variables are highly correlated between each other but, contrairly to the other variables, also show a significant correlation with the response variable, it made sense to plot the two variables in the following scatter plot to check for possible patterns.

```{r transactions_scatterplot, echo=FALSE}
# Plot of Total_Trans_Ct vs Total_Trans_Amt
ggplot(bank, aes(x = log(Total_Trans_Amt), y = Total_Trans_Ct, color = as.factor(Attrition_Flag))) +
  geom_point(alpha = .5) +
  scale_color_manual(values = c("0" = "royalblue", "1" = "#FF5733"),
                     name = "Attrition Flag:",
                     labels = c("Existing", "Attrited")) +
  labs(x = "Total Transaction Amount", y = "Total Transaction Count") +
  theme(legend.position = c(.85, .15),
        legend.background = element_rect(fill = "transparent"),
        legend.title = element_text(size = 10),
        aspect.ratio = 1)
```

The scatter plot represents the total number of transactions (on the y axis) agains the logarithm of the total transaction amount (on the x axis). The plot is colored by the `Attrition_Flag` variable, and we can indeed visually confirm the correlation among the two variables. However we can also spot that the churned customers are more concentrated in the lower left part of the plot.\
This fact is also reflected in the distribution analysis of the two variables reported below.
  
```{r trans_amt_distribution, echo=FALSE, fig.width=8, fig.height=8}
plot_continuous(bank, Total_Trans_Amt, "Total Transaction Amount", "Amount", 1000)
```

```{r trans_ct_distribution, fig.width=8, fig.height=8}
# plot_discrete(bank, Total_Trans_Ct, "Total Transaction Count", "Count")
plot_continuous(bank, Total_Trans_Ct, "Total Transaction Count", "Count", 5)
```

# Models implementation

Different models has been built to predict the `Attrition_Flag` variable and are presented in this section.
In order to assess the model performance, different effectiveness metrics have been used. These have been computed both by fitting the model using all the observations in the dataset and also by performing a k-fold cross validation with $k=10$. The metrics used are the following:

* Accuracy
* AUC
* FPR (False Positive Rate)
* FNR (False Negative Rate)
* Confusion matrix (*only for a single static split*)

The Dummy classifier has been taken as a baseline for comparison.

## Logistic regression

The first model we build in the attempt of predicting the response variable has been a logistic regression model using the $logit$ link function.\
In the initial model we included all the covariates except those that had low correlation with the response variable. We further filtered the covariates selection by discarding the variables that had a $p-value$ higher than 0.05 in the `glm()` summary output, i.e. the variables that were not statistically significant in the prediction.\
Possible multicollinearity problems, especially among the variables that showed high correlation in the exploratory analysis, were checked by looking at the Variance Inflation Factor (VIF) of the covariates.\
The final model has been built using the following variables:

* `Gender`: the gender of the customer
* `Marital_Status`: the marital status of the customer
* `Income_Category`: the income category of the customer
* `Total_Relationship_Count`: the total number of products held by the customer
* `Months_Inactive_12_mon`: the number of months inactive in the last 12 months
* `Contacts_Count_12_mon`: the number of contacts in the last 12 months
* `Total_Revolving_Bal`: the total revolving balance on the credit card
* `Total_Trans_Amt`: the total transaction amount in the last 12 months
* `Total_Trans_Ct`: the total transaction count in the last 12 months
* `Total_Ct_Chng_Q4_Q1`: the change in transaction count from Q4 to Q1

Aditionally, looking at the data distribution it has been taken the logarihmic values of the `Total_Trans_Amt` variable. Moreover the `Marital_Status` variable has been converted into a binary variable, taking the levels "Married" and "Not Married" since this change helped improving the statitical significane of the variable in the final model. For the same reason, the variable `Income_Category` has also been converted into a binary variable, dividing the clients in two categories: those with an income less than 120K and those with an income more than 120K.\
Both these changes have significantly improved the model performance as portrayed by the metrics explained below. An ANOVA test has also been performed to check the significance of the model.


```{r logistic_model_build, echo=FALSE}
# Rebuiding and checking the model
simple_logistic_model <- learn_logistic(bank_logistic)

```

### Results on the original dataset

On the given dataset, the built logistic model summary is the following:

```{r logistic_model_summary}
summary(simple_logistic_model)
```

The results of the ANOVA test are the following:

```{r logistic_model_anova}
anova(simple_logistic_model, test = "Chisq")
```

As its possible to see all the contribute in decreasing the deviance of the model, and they are all statistically significant.\

For what concerns the effectiveness metrics, we measured the following results on the original dataset:

```{r logistic model_results, message=FALSE, warning=FALSE}
logistic_results <- assess_logistic(simple_logistic_model, bank_logistic)
```

And similar results for the k-fold cross validation:

```{r logistic_model_cv_results, message=FALSE, warning=FALSE}
cv_logistic(bank_logistic)
```

From the results we can clearly see that the model is overall performing well. The accuracy is consistently over $90\%$ on all the folds, but this is not too surprising given the unbalance nature of the dataset. Good values have however been reached for the AUC and the FPR, with the worst metric being the FNR which falls just below $50\%$. The AIC and BIC values are also very low, which is a good sign for the model.

### Results on the synthetic dataset (ROSE)

Let's apply *ROSE* package:
```{r}
bank_logistic_balanced<- ROSE(Attrition_Flag~.,data=bank_logistic,seed = 123)$data
```

Now we can learn a logistic regression model on the new balanced dataset:

```{r}
ROSE_logistic_model <- learn_logistic(bank_logistic_balanced)
```

For what concerns the effectiveness metrics on the static trai test split:

```{r, message=FALSE, warning=FALSE}
ROSE_logistic_results<- assess_logistic(ROSE_logistic_model, bank_logistic_balanced)
```

Instead, by computing a 10-fold CV:
```{r, message=FALSE, warning=FALSE}
cv_logistic(bank_logistic_balanced)
```
**Comment:** the accuracy of this model, as expected, is lower than one of the logistic model learnt on the unbalanced dataset. However, the difference with respect to the dummy classifier becomes definitely more evident. Regarding the AUC score, even its value is slightly lower than before, it still showcases how good is the fit of our model. The metric that really improves after applying ROSE is the FNR: the model learnt on the unbalanced dataset had an average FNR of 44.11%, while now it drops around 17.87%. Conversely, FPR increases from 3.02% to 17.85%, but since we don't know anything about the costs of the errors, we considered it better to have more balanced metrics for FPR and FNR.

## GAM/Splines
We also tried using a General Additive Model to predict the Attrition_Flag variable. At first we included all the variables in the model, using a spline approximation for all the non categorical variables, obtaining an accuracy of 96,42% (AUC 98.99%). 
Than we removed from the model the variables not statistically significant and we included as linear variables whose spline approximation has an expected degree of freedom minor or equal to 2 obtaining an accuracy of 96.41% (AUC 98.93%). 
The final model has been built using as linear:

* `Customer_Age`: customer age in years
* `Gender`: the gender of the customer
* `Dependent_count`: number of dependents
* `Marital_Status`: married, not married
* `Total_Relationship_Count`: the total number of products held by the customer
* `Months_Inactive_12_mon`: the number of months inactive in the last 12 months
* `Contacts_Count_12_mon`: the number of contacts in the last 12 months
* `Total_Revolving_Bal`: the total revolving balance on the credit card
* `Total_Amt_Chng_Q4_Q1`: change in transaction amount
* `Total_Trans_Amt`: the total transaction amount in the last 12 months
* `Total_Trans_Ct`: the total transaction count in the last 12 months
* `Total_Ct_Chng_Q4_Q1`: the change in transaction count from Q4 to Q1

As in the logistic regression model we converted `Months_Inactive_12_mon` to a categorical variable with 4 categories: 1,2,3,4+; and we took the logarithmic values of `Total_Trans_Amt`.

The assessment has been done using the same procedure as the logistic regression model: we performed a k-fold cross validation with k=10, and we used the same metrics.

```{r gam_model_build, echo=FALSE}
# Set working directory as this directory
setwd(dirname(rstudioapi::getSourceEditorContext()$path))

# Load the dataset and pre-process it
bank_gam <- read.csv("./datasets/BankChurners.csv", sep = ",")

# ⚠️ Remove the last two columns as suggested in the README
bank_gam <- bank_gam[, -c(22, 23)]

# ⚠️ Remove the first column as it is just an index
bank_gam <- bank_gam[, -1]

# ⚠️ Convert the Attrition_Flag column to a binary variable:
# - 0: Existing Customer
# - 1: Attrited Customer
bank_gam$Attrition_Flag <- ifelse(bank_gam$Attrition_Flag == "Attrited Customer", 1, 0)

# Convert all categorical variables to factors
bank_gam$Gender <- as.factor(bank_gam$Gender)

bank_gam$Income_Category <- fct_collapse(bank_gam$Income_Category,
                                     "Less than 120K" = c("Unknown",
                                                          "Less than $40K",
                                                          "$40K - $60K",
                                                          "$60K - $80K",
                                                          "$80K - $120K"),
                                     "More than 120K" = c("$120K +"))

# Chnanging the levels of Marital_Status in either married or not married
bank_gam$Marital_Status <- fct_collapse(bank_gam$Marital_Status,
                                    "Married" = c("Married"),
                                    "Not Married" = c("Divorced",
                                                      "Single",
                                                      "Unknown"))
# Converting Months_Inactive_12_mon to a factor
bank_gam$Months_Inactive_12_mon <- as.factor(bank_gam$Months_Inactive_12_mon)
levels(bank_gam$Months_Inactive_12_mon) <- c("0", "1", "2", "3", "4", "5", "6+")

# Joining together the levels after 4 months
bank_gam$Months_Inactive_12_mon <- fct_collapse(bank_gam$Months_Inactive_12_mon,
                                            "4+" = c("4", "5", "6+"))

bank_gam$Education_Level <- as.factor(bank_gam$Education_Level)
bank_gam$Marital_Status <- as.factor(bank_gam$Marital_Status)
bank_gam$Income_Category <- as.factor(bank_gam$Income_Category)
bank_gam$Card_Category <- as.factor(bank_gam$Card_Category)
bank_gam$Total_Trans_Amt <- log(bank_gam$Total_Trans_Amt)
```

### Results on the original dataset

The build gamfit model is:

```{r gam_model_summary}
gamfit<-learn_gam(bank_gam)
summary(gamfit)
```
Regarding the effectiveness metrics we obtained:

```{r gam model_results, message=FALSE, warning=FALSE}
gam_results <- assess_gam(gamfit, bank_gam)
```
And similar results for the k-fold cross validation:

```{r gam_model_cv_results, message=FALSE, warning=FALSE}
cv_gam(bank_gam)
```

### Results on the synthetic dataset (ROSE)

First of all, let's obtain the synthetic dataset:
```{r,message=FALSE, warning=FALSE}
bank_gam_balanced<- ROSE(Attrition_Flag~.,data=bank_gam,seed = 123)$data
```

Learning phase of the model:
```{r,message=FALSE, warning=FALSE}
ROSE_gamfit<-learn_gam(bank_gam_balanced)
```


```{r,message=FALSE, warning=FALSE}
ROSE_gam_results<- assess_gam(ROSE_gamfit, bank_gam_balanced)
```

```{r, message=FALSE, warning=FALSE}
cv_gam(bank_gam_balanced)
```

## Decision Trees 

Here,we have tried to use the Decision Trees method via classification trees to consider modelling the predictor variable Attrition Flag.\
We started with preprocessing the dataset on similar lines as done before for previous models and converted target variable Attrition flag into \
binary variable (0 and 1) and converted relevant categorical variables into factors.We took the approach of building classification tree model by \
starting with dummy classifier (as a baseline model) followed by full tree and later optimising tree with less variables(reduced tree model),\
k-fold tree and finally with hyperparameter tuned model.

The final model (tuned one) has been built using the following variables:

* `Gender`: the gender of the customer
* `Dependent_count`: total dependents on the customer
* `Marital_Status`: the marital status of the customer
* `Income_Category`: the income category of the customer
* `Card_Category`: the Type of Card (Blue, Silver, Gold, Platinum)
* `Total_Relationship_Count`: the total number of products held by the customer
* `Months_Inactive_12_mon`: the number of months inactive in the last 12 months
* `Contacts_Count_12_mon`: the number of contacts in the last 12 months
* `Credit_Limit`: the credit Limit on the Credit Card
* `Total_Revolving_Bal`: the total revolving balance on the credit card
* `Total_Trans_Amt`: the total transaction amount in the last 12 months
* `Total_Trans_Ct`: the total transaction count in the last 12 months
* `Total_Ct_Chng_Q4_Q1`: the change in transaction count from Q4 to Q1
* `Total_Amt_Chng_Q4_Q1`: the change in transaction amount (Q4 over Q1)

# Pre-processing steps:

```
# Set working directory as this directory
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
# Load the dataset from the datasets/ folder
bank <- read.csv("./datasets/BankChurners.csv", sep = ",")
#Remove the last two columns as suggested in the README. Remove the first ID column.
bank <- bank[, -c(1, 22, 23)]
# Convert the Attrition_Flag column to a binary variable:
# - 0: Existing Customer
# - 1: Attrited Customer
bank$Attrition_Flag <- ifelse(bank$Attrition_Flag == "Attrited Customer", 1, 0)
# Convert all categorical variables to factors
bank$Attrition_Flag <- as.factor(bank$Attrition_Flag)
bank$Gender <- as.factor(bank$Gender)
bank$Education_Level <- as.factor(bank$Education_Level)
bank$Marital_Status <- as.factor(bank$Marital_Status)
bank$Income_Category <- as.factor(bank$Income_Category)
bank$Card_Category <- as.factor(bank$Card_Category)
```

# Perform partition on entire data to segregate train and test data.
```
set.seed(123)
trainIndex <- createDataPartition(bank$Attrition_Flag, p = 0.8, list = FALSE)
train_data <- bank[trainIndex, ]
validation_data <- bank[-trainIndex, ]
```

# Classification tree for dummy classifier
# Dummy classifier: Predict the majority class for all instances
```
majority_class_train <- levels(train_data$Attrition_Flag)[which.max(table(train_data$Attrition_Flag))]
train_data$dummy_predictions <- rep(majority_class_train, nrow(train_data))
majority_class_validation <- levels(validation_data$Attrition_Flag)[which.max(table(validation_data$Attrition_Flag))]
validation_data$dummy_predictions <- rep(majority_class_validation, nrow(validation_data)
# Build and print the confusion matrix
conf_matrix <- table(train_data$Attrition_Flag, dummy_predictions)
print(conf_matrix)
# Plot the dummy tree (for visualization purposes)
dummy_tree <- rpart(Attrition_Flag ~ dummy_predictions, data = train_data, method = "class")
rpart.plot(dummy_tree, extra=1, digits=4, box.palette="auto")
```
predictions_dummy <- predict(dummy_tree, type = "class", newdata = validation_data)


#Performance indices:
# Calculate accuracy
accuracy_dummy <- confusionMatrix(predictions_dummy, validation_data$Attrition_Flag)$overall['Accuracy']

# Calculate precision
precision_dummy <- confusionMatrix(predictions_dummy, validation_data$Attrition_Flag)$byClass['Precision']

# Calculate recall
recall_dummy <- confusionMatrix(predictions_dummy, validation_data$Attrition_Flag)$byClass['Sensitivity']

# Calculate specificity
specificity_dummy <- confusionMatrix(predictions_dummy, validation_data$Attrition_Flag)$byClass['Specificity']

#Calculate FPR
fpr_dummy <- round(1-specificity_dummy,2)*100

#Calculate FNR
fnr_dummy <- round(1-recall_dummy,2)*100

# Calculate F1 score
f1_score_dummy <- confusionMatrix(predictions_dummy, validation_data$Attrition_Flag)$byClass['F1']

# Calculate AUC-ROC

roc_dummy <- roc(validation_data$Attrition_Flag, as.numeric(predictions_dummy))
auc_roc_dummy <- auc(roc_dummy)

original_performance_dummy <- c(Accuracy = round(accuracy_dummy*100,digits=2), Precision = round(precision_dummy*100,digits=2), Recall = round(recall_dummy*100,digits=2), 
                               FPR=fpr_dummy, FNR=fnr_dummy,Specificity = round(specificity_dummy*100,digits=2), F1_Score = round(f1_score_dummy*100,digits=2), AUC_ROC = round(auc_roc_dummy*100,digits=2))

original_performance_dummy
```

# Classification tree considering all predictors and response = Attrition_Flag
# Full tree
```
fit <- rpart(Attrition_Flag ~ ., method="class", data = train_data)
rpart.plot(fit, extra=1, digits=4, box.palette="auto")
```
```
predictions <- predict(fit, type = "class", newdata = validation_data)

```
#Performance indices:
# Calculate accuracy
accuracy <- confusionMatrix(predictions, validation_data$Attrition_Flag)$overall['Accuracy']
# Calculate precision
precision <- confusionMatrix(predictions, validation_data$Attrition_Flag)$byClass['Precision']
# Calculate recall
recall <- confusionMatrix(predictions, validation_data$Attrition_Flag)$byClass['Sensitivity']
# Calculate specificity
specificity <- confusionMatrix(predictions, validation_data$Attrition_Flag)$byClass['Specificity']
# Calculate F1 score
f1_score <- confusionMatrix(predictions, validation_data$Attrition_Flag)$byClass['F1']
# Calculate AUC-ROC
roc <- roc(validation_data$Attrition_Flag, as.numeric(predictions))
auc_roc <- auc(roc)
original_performance <- c(Accuracy = accuracy, Precision = precision, Recall = recall, 
                          Specificity = specificity, F1_Score = f1_score, AUC_ROC = auc_roc)
original_performance
```
# Now, let's consider the logistic regression to decide the significant variables from glm based on p-values

#LR model to select significant variables and then use those variables for classification tree

lr1 <- glm(Attrition_Flag~., data = bank, family = binomial)
summary(lr1)

# Summary shows NA values also. This is case when alias are present. Let's check alias. 
alias(lr1)

# The variable "Avg_Open_To_But" is alias. We will remove this and run LR. 

bank_no_alias <- bank[, -15] # creating data removing alias for running logistic regression. 

lr2 <- glm(Attrition_Flag~., data = bank_no_alias, family = binomial)
summary(lr2)

#From the summary of lr2 model, the significant variables are:
#Gender, Dependent_count,  Marital_Status,  Income_Category, Card_Category, Total_Relationship_Count,
#Months_Inactive_12_mon, Contacts_Count_12_mon, Credit_Limit,Total_Revolving_Bal, 
#Total_Amt_Chng_Q4_Q1, Total_Trans_Amt, Total_Trans_Ct, Total_Ct_Chng_Q4_Q1

## check multicollinearity

# Check VIF for the new model

vif_values <- car::vif(lr2)

print(vif_values)

# There doesn't seems to be multicollinearity issue. 

# clasification tree - reduced model, selected from lr2 model

train_data <- train_data[,c("Attrition_Flag", "Gender", "Dependent_count", "Marital_Status", "Income_Category", "Card_Category", 
                        "Total_Relationship_Count", "Months_Inactive_12_mon", "Contacts_Count_12_mon", "Credit_Limit", 
                        "Total_Revolving_Bal", "Total_Amt_Chng_Q4_Q1", "Total_Trans_Amt","Total_Trans_Ct","Total_Ct_Chng_Q4_Q1")]

validation_data <- validation_data[,c("Attrition_Flag", "Gender", "Dependent_count", "Marital_Status", "Income_Category", "Card_Category", 
                            "Total_Relationship_Count", "Months_Inactive_12_mon", "Contacts_Count_12_mon", "Credit_Limit", 
                            "Total_Revolving_Bal", "Total_Amt_Chng_Q4_Q1", "Total_Trans_Amt","Total_Trans_Ct","Total_Ct_Chng_Q4_Q1")]


fit1 <- rpart(Attrition_Flag ~ Gender + Dependent_count + Marital_Status + Income_Category + Card_Category + Total_Relationship_Count + 
               Months_Inactive_12_mon + Contacts_Count_12_mon + Credit_Limit + Total_Revolving_Bal + 
                Total_Amt_Chng_Q4_Q1 + Total_Trans_Amt + Total_Trans_Ct + Total_Ct_Chng_Q4_Q1, method="class", data = train_data)
plotcp(fit1) #gives cp parameters later to be chosen for hyperparameter tuning

rpart.plot(fit1, extra=1, digits=4, box.palette="auto")

pruned_tree <- prune(fit1, cp = 0.011)
rpart.plot(pruned_tree, extra=1, digits=4, box.palette="auto")
# Plot the pruned tree
plot(pruned_tree)
text(pruned_tree, use.n = TRUE)

predictions_rm <- predict(fit1, type = "class", newdata = validation_data)
table(predictions_rm)
#Performance indices:

# Calculate accuracy
accuracy_rm <- confusionMatrix(predictions_rm, validation_data$Attrition_Flag)$overall['Accuracy']

# Calculate precision
precision_rm <- confusionMatrix(predictions_rm, validation_data$Attrition_Flag)$byClass['Precision']

# Calculate recall
recall_rm <- confusionMatrix(predictions_rm, validation_data$Attrition_Flag)$byClass['Sensitivity']

# Calculate specificity
specificity_rm <- confusionMatrix(predictions_rm, validation_data$Attrition_Flag)$byClass['Specificity']

# Calculate F1 score
f1_score_rm <- confusionMatrix(predictions_rm, validation_data$Attrition_Flag)$byClass['F1']

# Calculate AUC-ROC
roc_rm <- roc(validation_data$Attrition_Flag, as.numeric(predictions_rm))
auc_roc_rm <- auc(roc_rm)

rm_performance <- c(Accuracy = accuracy_rm, Precision = precision_rm, Recall = recall_rm, 
                          Specificity = specificity_rm, F1_Score = f1_score_rm, AUC_ROC = auc_roc_rm)

rm_performance


#k-fold cross validation


ctrl <- trainControl(method = "cv",  # Use k-fold cross-validation
                     number = 10)     # Number of folds (e.g., 10-fold)


# Perform k-fold cross-validation
set.seed(123)
cv <- train(Attrition_Flag ~ Gender + Dependent_count + Marital_Status + Income_Category + Card_Category + Total_Relationship_Count + 
              Months_Inactive_12_mon + Contacts_Count_12_mon + Credit_Limit + Total_Revolving_Bal + 
              Total_Amt_Chng_Q4_Q1 + Total_Trans_Amt + Total_Trans_Ct + Total_Ct_Chng_Q4_Q1, data = train_data, method = "rpart", trControl = ctrl)      

# View the cross-validation results
print(cv)

predictions_kfold <- predict(cv, type = "raw", newdata = validation_data)

table(predictions_kfold)
#Performance indices:

# Calculate accuracy
accuracy_kfold <- confusionMatrix(predictions_kfold, validation_data$Attrition_Flag)$overall['Accuracy']

# Calculate precision
precision_kfold <- confusionMatrix(predictions_kfold, validation_data$Attrition_Flag)$byClass['Precision']

# Calculate recall
recall_kfold <- confusionMatrix(predictions_kfold, validation_data$Attrition_Flag)$byClass['Sensitivity']

# Calculate specificity
specificity_kfold <- confusionMatrix(predictions_kfold, validation_data$Attrition_Flag)$byClass['Specificity']

# Calculate F1 score
f1_score_kfold <- confusionMatrix(predictions_kfold, validation_data$Attrition_Flag)$byClass['F1']

# Calculate AUC-ROC
roc_kfold <- roc(validation_data$Attrition_Flag, as.numeric(predictions_kfold))
auc_roc_kfold <- auc(roc_kfold)

kfold_performance <- c(Accuracy = accuracy_kfold, Precision = precision_kfold, Recall = recall_kfold, 
                    Specificity = specificity_kfold, F1_Score = f1_score_kfold, AUC_ROC = auc_roc_kfold)

kfold_performance

#Accuracy for reduced model fit2 without performing kfold is better. So, next we try tuning a tree of reduced model. 

#Hyperparameter tuning

# Define parameters for tuning
ctrl <- rpart.control(minsplit = 4,
                         minbucket = round(5 / 3),
                         maxdepth = 3,
                         cp = 0.011)



# Perform tuning model
fit_tune <- rpart(Attrition_Flag ~ Gender + Dependent_count + Marital_Status + Income_Category + Card_Category + Total_Relationship_Count + 
                    Months_Inactive_12_mon + Contacts_Count_12_mon + Credit_Limit + Total_Revolving_Bal + 
                    Total_Amt_Chng_Q4_Q1 + Total_Trans_Amt + Total_Trans_Ct + Total_Ct_Chng_Q4_Q1, method = "class", data = train_data, control = ctrl)
# Evaluate performance
print(fit_tune)

rpart.plot(fit_tune, extra=1, digits=4, box.palette="auto")

# print summary of best model
#summary(best_model)

# Access the split nodes starting from the root
#split_nodes <- best_model$splits[, "variable"]
#unique_split_nodes <- unique(split_nodes)
# Print the unique split nodes
#print(unique_split_nodes)



#split_variables <- unique(best_model$variable)


predictions_tune <- predict(fit_tune, newdata = validation_data, type = "class")

levels(predictions_tune)
table(predictions_tune)

dim(data.frame(predictions_tune))

#Performance indices:

# Calculate accuracy
accuracy_tune <- confusionMatrix(predictions_tune, validation_data$Attrition_Flag)$overall['Accuracy']

# Calculate precision
precision_tune <- confusionMatrix(predictions_tune, validation_data$Attrition_Flag)$byClass['Precision']

# Calculate recall
recall_tune <- confusionMatrix(predictions_tune, validation_data$Attrition_Flag)$byClass['Sensitivity']

# Calculate specificity
specificity_tune <- confusionMatrix(predictions_tune, validation_data$Attrition_Flag)$byClass['Specificity']

# Calculate F1 score
f1_score_tune <- confusionMatrix(predictions_tune, validation_data$Attrition_Flag)$byClass['F1']

# Calculate AUC-ROC
roc_tune <- roc(validation_data$Attrition_Flag, as.numeric(predictions_tune))
auc_roc_tune <- auc(roc_tune)

tune_tree_performance <- c(Accuracy = accuracy_tune, Precision = precision_tune, Recall = recall_tune, 
                       Specificity = specificity_tune, F1_Score = f1_score_tune, AUC_ROC = auc_roc_tune)

tune_tree_performance

###Random Forest check
library(randomForest)
randomforest_model <- randomForest(Attrition_Flag ~ ., data = train_data, ntree = 100, mtry = 4)
#validation_data$rf_predictions <- predict(randomforest_model, newdata = test_data)
plot(randomforest_model)
predictions_rf <- predict(randomforest_model, newdata = validation_data, type = "class")

levels(predictions_rf)
table(predictions_rf)

dim(data.frame(predictions_rf))

#Performance indices:

# Calculate accuracy
accuracy_rf <- confusionMatrix(predictions_rf, validation_data$Attrition_Flag)$overall['Accuracy']

# Calculate precision
precision_rf <- confusionMatrix(predictions_rf, validation_data$Attrition_Flag)$byClass['Precision']

# Calculate recall
recall_rf <- confusionMatrix(predictions_rf, validation_data$Attrition_Flag)$byClass['Sensitivity']

# Calculate specificity
specificity_rf <- confusionMatrix(predictions_rf, validation_data$Attrition_Flag)$byClass['Specificity']

# Calculate F1 score
f1_score_rf <- confusionMatrix(predictions_rf, validation_data$Attrition_Flag)$byClass['F1']

# Calculate AUC-ROC
roc_rf <- roc(validation_data$Attrition_Flag, as.numeric(predictions_rf))
auc_roc_rf <- auc(roc_rf)

rf_tree_performance <- c(Accuracy = accuracy_rf, Precision = precision_rf, Recall = recall_rf, 
                           Specificity = specificity_rf, F1_Score = f1_score_rf, AUC_ROC = auc_roc_rf)

rf_tree_performance


##random forest ends here

test_data$rf_predictions <- predict(randomforest_model, newdata = test_data)

rf_metrics <- calculate_metrics(test_data$Total_points, test_data$rf_predictions)



# Checking balance in response variable:

table(bank$Attrition_Flag)

#There is high imbalance in data. Applying ROSE() to get balanced data

# cat("Attrited customers (rare class): ",sum(bank_logistic$Attrition_Flag==1))
# cat("Existing customers (maj class): ",sum(bank_logistic$Attrition_Flag==0))
# cat("Proportion of attrited:",
#     sum(bank_logistic$Attrition_Flag==1)/sum(table(bank_logistic$Attrition_Flag))*100,"%")
bank_ROSE <- bank[,c("Attrition_Flag", "Gender", "Dependent_count", "Marital_Status", "Income_Category", "Card_Category", 
                           "Total_Relationship_Count", "Months_Inactive_12_mon", "Contacts_Count_12_mon", "Credit_Limit", 
                           "Total_Revolving_Bal", "Total_Amt_Chng_Q4_Q1", "Total_Trans_Amt","Total_Trans_Ct","Total_Ct_Chng_Q4_Q1")]
#install.packages("ROSE")
library(ROSE)
bank_balanced<- ROSE(Attrition_Flag ~ ., data=bank_ROSE, seed = 123)$data

table(bank_balanced$Attrition_Flag)


#Now we can run a classification tree model on the new balanced dataset:

ROSE_tree_fit <- rpart(Attrition_Flag ~ ., method="class", data = bank_balanced)

rpart.plot(ROSE_tree_fit, extra=1, digits=4, box.palette="auto")

predictions_ROSE <- predict(ROSE_tree_fit, type = "class", newdata= bank_balanced)

#Performance indices:

# Calculate accuracy
accuracy_ROSE <- confusionMatrix(predictions_ROSE, bank_balanced$Attrition_Flag)$overall['Accuracy']

# Calculate precision
precision_ROSE <- confusionMatrix(predictions_ROSE, bank_balanced$Attrition_Flag)$byClass['Precision']

# Calculate recall
recall_ROSE <- confusionMatrix(predictions_ROSE, bank_balanced$Attrition_Flag)$byClass['Sensitivity']

# Calculate specificity
specificity_ROSE <- confusionMatrix(predictions_ROSE, bank_balanced$Attrition_Flag)$byClass['Specificity']

# Calculate F1 score
f1_score_ROSE <- confusionMatrix(predictions_ROSE, bank_balanced$Attrition_Flag)$byClass['F1']

# Calculate AUC-ROC

roc_ROSE <- roc(bank_balanced$Attrition_Flag, as.numeric(predictions_ROSE))
auc_roc_ROSE <- auc(roc_ROSE)

ROSE_performance <- c(Accuracy = accuracy_ROSE, Precision = precision_ROSE, Recall = recall_ROSE, 
                          Specificity = specificity_ROSE, F1_Score = f1_score_ROSE, AUC_ROC = auc_roc_ROSE)

ROSE_performance


# plotting of performance indices for different trees

all_trees <- data.frame(
  Method = rep(c("Full_tree","Reduced_tree","kfold_tree","Tuned_tree", "ROSE_tree"), each = 6),
  Metric = rep(c("Accuracy", "Precision","Recall", "Specificity", "F1_Score", "AUC_ROC"), times = 5),
  Value = c(original_performance, rm_performance, kfold_performance, tune_tree_performance, ROSE_performance)
)

ggplot(all_trees, aes(x = Method, y = Value, fill = Method)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~Metric, scales = "free_y") +
  labs(title = "Comparison of Classification Tree Methods",
       x = "Method", y = "Value") +
  theme_minimal()


## Ensemble Methods

The last class of methods we used to model the `Attrition_Flag` variable is ensamble methods, in particular we focussed on AdaBoost and Random Forest.\
The first model we built is an AdaBoost model on the whole dataset, which on its own achieved some great results. Then we proceeded by removing the variables considered not statistically significant in more complex models like `GAM`. This slight modification didn't much modify the accuracy of the model, which is always on the order of about $\sim90\%$.\
The final model has been built using the following variables:

* `Gender`: the gender of the customer
* `Total_Relationship_Count`: the total number of products held by the customer
* `Months_Inactive_12_mon`: the number of months inactive in the last 12 months
* `Contacts_Count_12_mon`: the number of contacts in the last 12 months
* `Total_Revolving_Bal`: the total revolving balance on the credit card
* `Total_Trans_Amt`: the total transaction amount in the last 12 months
* `Total_Trans_Ct`: the total transaction count in the last 12 months
* `Total_Ct_Chng_Q4_Q1`: the change in transaction count from Q4 to Q1
* `Marital_Status`: the marital status of the customer
* `Income_Category`: the income category of the customer

The same pre-processing steps of the `GLM` model were performed, slightly modifying classes or taking logarithmic values and converting the `Months_Inactive_12_mon` variable to a factor.\
The effectiveness metrics used to assess the model are:

* Accuracy
* AUC
* FPR (False Positive Rate)
* FNR (False Negative Rate)
* Confusion matrix
* Variable importance

The dummy classifier has been taken as a baseline for comparison.

```{r ada_model_build, echo=FALSE}
setwd(dirname(rstudioapi::getSourceEditorContext()$path))

# Load the dataset and pre-process it
bank_ensamble <- read.csv("datasets/BankChurners.csv", sep = ",")
bank_ensamble <- bank_ensamble[, -c(1, 22, 23)]
#bank$Attrition_Flag <- ifelse(bank$Attrition_Flag == "Attrited Customer", 1, 0)
# Convert Attrition_Flag to a binary factor
bank_ensamble$Attrition_Flag <- factor(bank_ensamble$Attrition_Flag == "Attrited Customer", levels = c(FALSE, TRUE))

# If "Attrited Customer" is TRUE, it will be coded as 1, and other values will be coded as 0


# Convert all categorical variables to factors and reorder the levels
bank_ensamble$Gender <- as.factor(bank_ensamble$Gender)
bank_ensamble$Income_Category <- fct_collapse(bank_ensamble$Income_Category,
                                     "Less than 120K" = c("Unknown",
                                                          "Less than $40K",
                                                          "$40K - $60K",
                                                          "$60K - $80K",
                                                          "$80K - $120K"),
                                     "More than 120K" = c("$120K +"))

# Changing the levels of Marital_Status in either married or not married
bank_ensamble$Marital_Status <- fct_collapse(bank_ensamble$Marital_Status,
                                    "Married" = c("Married"),
                                    "Not Married" = c("Divorced",
                                                      "Single",
                                                      "Unknown"))
# Converting Months_Inactive_12_mon to a factor
bank_ensamble$Months_Inactive_12_mon <- as.factor(bank_ensamble$Months_Inactive_12_mon)
levels(bank_ensamble$Months_Inactive_12_mon) <- c("0", "1", "2", "3", "4", "5", "6+")

# Joining together the levels after 4 months
bank_ensamble$Months_Inactive_12_mon <- fct_collapse(bank_ensamble$Months_Inactive_12_mon,
                                            "4+" = c("4", "5", "6+"))

bank_ensamble$Education_Level <- as.factor(bank_ensamble$Education_Level)
bank_ensamble$Marital_Status <- as.factor(bank_ensamble$Marital_Status)
bank_ensamble$Income_Category <- as.factor(bank_ensamble$Income_Category)
bank_ensamble$Card_Category <- as.factor(bank_ensamble$Card_Category)
# Override the Total_Trans_Amt variable with its log !!!
bank_ensamble$Total_Trans_Amt <- log(bank_ensamble$Total_Trans_Amt)
bank_ensamble <- bank_ensamble[, -c(2, 4, 5, 8, 9, 13, 15, 16, 20)]

# Actual model building
set.seed(1234)
index <- createDataPartition(bank_ensamble$Attrition_Flag , p =0.8, list = FALSE)

train_bank_ensamble <- bank_ensamble[index,]
test_bank_ensamble <- bank_ensamble[-index,]
```

Learning phase:
```{r learning_boost_rf,message=FALSE,warning=FALSE}
boost_model <- learn_boost(train_bank_ensamble)
rf_model <- learn_rf(train_bank_ensamble)
```

### Results on the original dataset
A summary-like output of the model doesn't exist, but we can still assess the model performance using the metrics described above.

First we show the results relative to a single run of the model on the dataset:

```{r ada_model_summary,message=FALSE,warning=FALSE}
boost_results <- assess_boost(boost_model, test_bank_ensamble)
```

Then the results on a 10-fold cross validation:

```{r ada_model_cv_results,message=FALSE,warning=FALSE}
cv_boost_results <- cv_boost(bank_ensamble)
```

As we can see from these results, AdaBoost is performing pretty well, having a consistent accuracy of over $90\%$ and a good AUC score. The FPR and FNR are also very low, keeping them consistently under $20\%$. Not having an AIC or BIC-like score, we can't really compare the model to the previous ones, but the results on classification are much better than the ones above, in particular with respect to the simple logistic regression.\

Passing to Random Forest, we perform the same analysis:

```{r rf_model_summary,message=FALSE,warning=FALSE}
rf_results <- assess_rf(rf_model, test_bank_ensamble)
```

```{r rf_model_cv_results,message=FALSE,warning=FALSE}
cv_rf_results <- cv_rf(bank_ensamble)
```

The results are very similar to the ones of AdaBoost, with the only difference being a slightly lower accuracy and AUC score. The FPR and FNR are also very low, keeping them consistently under $20\%$. A consistent note should be pointed that on the fact that performing cross validation, the average FNR decreases to below $5\%$, demonstrating the adaptive power of Random Forest also on unbalanced data.\


### Results on the synthetic dataset (ROSE)

From `bank_ensamble` let's obtain a new (synthetic) dataset:

```{r,message=FALSE,warning=FALSE}
bank_ensamble_balanced<- ROSE(Attrition_Flag~.,data=bank_ensamble,seed = 123)$data
```

It is now splitted into a training and test set (to be consistent, we use the same seed used before)
```{r,message=FALSE,warning=FALSE}
set.seed(1234)
index <- createDataPartition(bank_ensamble_balanced$Attrition_Flag , p =0.8, list = FALSE)
train_bank_ensamble_balanced <- bank_ensamble_balanced[index,]
test_bank_ensamble_balanced <- bank_ensamble_balanced[-index,]
```

Learning phase:
```{r,message=FALSE,warning=FALSE}
ROSE_boost_model <- learn_boost(train_bank_ensamble_balanced)
ROSE_rf_model <- learn_rf(train_bank_ensamble_balanced)
```

Assessment with static train/test division:

```{r,message=FALSE,warning=FALSE}
assess_boost(ROSE_boost_model, test_bank_ensamble_balanced)
```

```{r,message=FALSE,warning=FALSE}
assess_rf(ROSE_rf_model, test_bank_ensamble_balanced)
```
    

Assessment with 10-fold cross validation:

```{r,message=FALSE,warning=FALSE}
cv_boost(bank_ensamble_balanced)
```

```{r,message=FALSE,warning=FALSE}
cv_rf(bank_ensamble_balanced)
```
  
# Interpretations/Conclusions

| Models | Accuracy|  AUC | FPR | FNR | ROSE | interpetability|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| Logistic regression |  91.36 % | 93.44 % | 3.15 % | 46.1 % | NO | interpetability|
|  | 81.41  %| 89.49  % | 18.44  % | 18.85  % | SI | interpetability|
| GAM |  96.19  % | 98.79 % | 1.84 % | 14.62 % | NO | interpetability|
|  |  84.48   % | 92.42  % | 17.22  % | 13.99  % | SI | interpetability|
| Decision Trees |  acc |AUC | FPR | FNR | ROSE | interpetability|
| Boost| 95.8 % |  98.58 % | 2.14 % | 14.94 % | NO | interpetability|
|  | 88.29 % |  95.17 % | 12.1 % | 11.31 % | SI | interpetability|
| Random Forest |  95.88 % | 98.53 % | 1.65 % | 18.63 % | NO | interpetability|
|   | 88.7 % |  95.22 %  | 11.64 % | 10.98  % | SI | interpetability|
| Logistic regression | acc|  AUC | FPR | FNR | ROSE | interpetability|

|                 Variable | Mean_Gini_Decrease |
|          Total_Trans_Amt |        594.260721 |
|           Total_Trans_Ct |        547.113190 |
|      Total_Revolving_Bal |        417.708629 |
|      Total_Ct_Chng_Q4_Q1 |        381.125419 |
| Total_Relationship_Count |        229.326038 |
|    Contacts_Count_12_mon |        100.106859 |
|   Months_Inactive_12_mon |         95.318035 |
|                   Gender |         41.048829 |
|           Marital_Status |         28.676287 |
|          Income_Category |          8.953285 |
